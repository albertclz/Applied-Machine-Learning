{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.preprocessing import StandardScaler, Imputer, PolynomialFeatures, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "data2015 = pd.read_excel('/Users/albertzhang/Desktop/18spring/Applied_ML/HW/HW_3/2015 FE Guide-for DOE-Mobility Ventures only-OK to release-no-sales-4-27-2017Mercedesforpublic.xlsx')\n",
    "data2016 = pd.read_excel('/Users/albertzhang/Desktop/18spring/Applied_ML/HW/HW_3/2016 FE Guide for DOE-OK to release-no-sales-4-27-2017Mercedesforpublic.xlsx')\n",
    "data2017 = pd.read_excel('/Users/albertzhang/Desktop/18spring/Applied_ML/HW/HW_3/2017 FE Guide for DOE-release dates before 9-20-2017-no sales-9-19-2017MercedesCadillacforpublic.xlsx')\n",
    "data2018 = pd.read_excel('/Users/albertzhang/Desktop/18spring/Applied_ML/HW/HW_3/2018 FE Guide for DOE-release dates before 2-24-2018-no-sales-2-23-2018public.xlsx')\n",
    "\n",
    "\n",
    "trainData = data2015.append([data2016, data2017],ignore_index=True)\n",
    "\n",
    "y_train = trainData['Comb Unrd Adj FE - Conventional Fuel'].to_frame()\n",
    "X_train = trainData.drop(columns=['Comb Unrd Adj FE - Conventional Fuel'])\n",
    "y_test = data2018['Comb Unrd Adj FE - Conventional Fuel'].to_frame()\n",
    "X_test = data2018.drop(columns=['Comb Unrd Adj FE - Conventional Fuel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clarificationï¼šBecause after onehotencoding, the categorical features got quite large amount, all models don't use GridResearch to tune the parameters due to the large time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Four linear models are applied in this task:\n",
    "\n",
    "1. Linear Regression\n",
    "\n",
    "2. Ridge\n",
    "\n",
    "3. Lasso\n",
    "\n",
    "4. Elastic Net\n",
    "\n",
    "The best model is Linear Regression.\n",
    "\n",
    "## 2. Pre-processing\n",
    "\n",
    "1. As required, first delete columns which have a strong correaltion with y: any columns has words ['EPA','CO2','Smog','Guzzler','FE','MPG','Cost','Rating','Range']\n",
    "\n",
    "2. Drop columns with more than 1/3 of records are nan, since these columns don't have some useful information for our prediction.\n",
    "\n",
    "\n",
    "#### The left columns will be divided into continuous columns and categorical columns.\n",
    "\n",
    "Continuous Columns: \n",
    "\n",
    "Model Year\n",
    "Index (Model Type Index)\n",
    "Eng Displ\n",
    "\\# Cyl\n",
    "\\# Gears\n",
    "Max Ethanol % - Gasoline\n",
    "Intake Valves Per Cyl\n",
    "Exhaust Valves Per Cyl\n",
    "Carline Class\n",
    "$ You Spend over 5 years (increased amount spent in fuel costs over 5 years - on label) \n",
    "\n",
    "For these columns:\n",
    "\n",
    "1. Impute with mean.\n",
    "\n",
    "2. Scaling with standardscaler.\n",
    "\n",
    "Categorical Columns:\n",
    "\n",
    "Mfr Name\n",
    "Division\n",
    "Carline\n",
    "Verify Mfr Cd\n",
    "Transmission\n",
    "Air Aspiration Method Desc\n",
    "Trans\n",
    "Trans Desc\n",
    "Lockup Torque Converter\n",
    "Trans Creeper Gear\n",
    "Drive Sys\n",
    "Drive Desc\n",
    "Fuel Usage  - Conventional Fuel\n",
    "Fuel Usage Desc - Conventional Fuel\n",
    "Fuel Unit - Conventional Fuel\n",
    "Fuel Unit Desc - Conventional Fuel\n",
    "Descriptor - Model Type (40 Char or less)\n",
    "Carline Class Desc\n",
    "Calc Approach Desc\n",
    "Release Date\n",
    "Unique Label?\n",
    "Label Recalc?\n",
    "Suppressed?\n",
    "Police/Emerg?\n",
    "Cyl Deact?\n",
    "Var Valve Timing?\n",
    "Var Valve Timing Desc\n",
    "Var Valve Lift?\n",
    "Fuel Metering Sys Cd\n",
    "Fuel Metering Sys Desc\n",
    "Camless Valvetrain (Y or N)\n",
    "Oil Viscosity\n",
    "Stop/Start System (Engine Management System) Code\n",
    "Stop/Start System (Engine Management System)  Description\n",
    "\n",
    "For these columns:\n",
    "\n",
    "1. Use get_dummies to do onehotencoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ColumnsType(X_train):\n",
    "    ### divide the columns into continuous and categorical\n",
    "    CategoricalColumns = list()\n",
    "    ContinuousColumns = list()\n",
    "    for column in X_train.columns:\n",
    "        if type(X_train[column].dropna().reset_index(drop=True)[0]) != np.int64 and type(X_train[column].dropna().reset_index(drop=True)[0]) != np.float64 and type(X_train[column].dropna().reset_index(drop=True)[0]) != int and type(X_train[column].dropna().reset_index(drop=True)[0]) != float:\n",
    "            CategoricalColumns.append(column)\n",
    "        else:\n",
    "            ContinuousColumns.append(column)\n",
    "    return ContinuousColumns,CategoricalColumns\n",
    "\n",
    "def preprocessing(X_train):\n",
    "    \n",
    "    ### delete direct measurement columns\n",
    "    keywords=['EPA','CO2','Smog','Guzzler','FE','MPG','Cost','Rating','Range']\n",
    "    for word in keywords:\n",
    "        for column in X_train.columns:\n",
    "            if word in column:\n",
    "                del X_train[column]\n",
    "    print(X_train.shape)\n",
    "                \n",
    "    ### drop columns with the number of value more than 2/3 times of the whole dataset\n",
    "    X_train = X_train.dropna(thresh=len(X_train)*(2/3), axis=1)\n",
    "    ####### remove value 'Mod'\n",
    "    \n",
    "    \n",
    "    modColumns=['MFR Calculated Gas Guzzler MPG ','FE Rating (1-10 rating on Label)','GHG Rating (1-10 rating on Label)','#1 Mfr Smog Rating (Mfr Smog 1-10 Rating on Label for Test Group 1)','City Unadj FE - Conventional Fuel','Hwy Unadj FE - Conventional Fuel','Comb Unadj FE - Conventional Fuel']\n",
    "    for column in modColumns:\n",
    "        try:\n",
    "            X_train[column] = X_train[column].replace(to_replace='Mod',value=np.nan)\n",
    "        except:\n",
    "            continue\n",
    "    print('strat to detect categorical columns')\n",
    "    print(X_train.shape)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def imputScalOHE(X_train,X_test,ContinuousColumns,CategoricalColumns):\n",
    "    '''\n",
    "    OneHotEncoding on categorical columns;\n",
    "    Imputation and Scaling on continuous columns\n",
    "    '''    \n",
    "    X_train_con = X_train[ContinuousColumns]\n",
    "    X_train_cat = X_train[CategoricalColumns]\n",
    "    del X_train_cat['Release Date']\n",
    "    X_test_con = X_test[ContinuousColumns]\n",
    "    X_test_cat = X_test[CategoricalColumns]\n",
    "    del X_test_cat['Release Date']\n",
    "    #####onehotencoding\n",
    "    X_train_cat = pd.get_dummies(X_train_cat)\n",
    "    X_test_cat = pd.get_dummies(X_test_cat)\n",
    "    # Get missing columns in the training test\n",
    "    missing_cols = set( X_train_cat.columns ) - set(X_test_cat.columns )\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for column in missing_cols:\n",
    "        X_test_cat[column] = 0\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    X_test_cat = X_test_cat[X_train_cat.columns]\n",
    "    #####impute\n",
    "    imputer = Imputer()\n",
    "    imputer.fit(X_train_con)\n",
    "    X_train_con_imputed = imputer.transform(X_train_con)\n",
    "    X_test_con_imputed = imputer.transform(X_test_con)\n",
    "    #####scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_con_imputed)    \n",
    "    X_train_con_scaled_imputed = scaler.transform(X_train_con_imputed) \n",
    "    X_test_con_scaled_imputed = scaler.transform(X_test_con_imputed) \n",
    "    \n",
    "    X_train_ISO =  np.concatenate((X_train_con_scaled_imputed,X_train_cat.as_matrix()),axis=1)\n",
    "    X_test_ISO = np.concatenate((X_test_con_scaled_imputed,X_test_cat.as_matrix()),axis=1)\n",
    "    \n",
    "    ConColsNum, CatColsNum = X_train_con.shape[1], X_train_cat.shape[1]\n",
    "    ##### ISO stands for Imputation Standardlization and Onehotencoding\n",
    "    return X_train_ISO, X_test_ISO, ConColsNum, CatColsNum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3701, 100)\n",
      "strat to detect categorical columns\n",
      "(3701, 44)\n",
      "(1227, 100)\n",
      "strat to detect categorical columns\n",
      "(1227, 44)\n",
      "LRScore:0.9485945527947737\n",
      "RidgeScore:0.9454906815505151\n",
      "LassoScore:0.537687474274948\n",
      "ElasticNetScore:0.5668373813115083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_prep = preprocessing(X_train)\n",
    "X_test_prep = preprocessing(X_test)\n",
    "ContinuousColumns, CategoricalColumns = ColumnsType(X_train_prep)\n",
    "\n",
    "\n",
    "X_train_ISO,X_test_ISO,ConColsNum,CatColsNum = imputScalOHE(X_train_prep,X_test_prep,ContinuousColumns,CategoricalColumns)\n",
    "\n",
    "\n",
    "LR=LinearRegression().fit(X_train_ISO,y_train)\n",
    "RG=Ridge().fit(X_train_ISO,y_train)\n",
    "LA=Lasso().fit(X_train_ISO,y_train)\n",
    "EN=ElasticNet().fit(X_train_ISO,y_train)\n",
    "\n",
    "print('LRScore:{}\\nRidgeScore:{}\\nLassoScore:{}\\nElasticNetScore:{}'.format(LR.score(X_test_ISO,y_test),RG.score(X_test_ISO,y_test),LA.score(X_test_ISO,y_test),EN.score(X_test_ISO,y_test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the GridResearch to find the best hyperparameters, the linear regression model already got mor than 0.94 R^2 value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Beside the features we used in task 1, in this task, we do the polynomial features on the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, ConColsNum, CatColsNum):\n",
    "    X_con = X[:,:ConColsNum]\n",
    "    X_cat = X[:,:(CatColsNum+ConColsNum)]\n",
    "    \n",
    "    poly = PolynomialFeatures()\n",
    "    X_con_poly = poly.fit_transform(X_con)  \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_con_poly)    \n",
    "    X_con_poly_scaled = scaler.transform(X_con_poly)\n",
    "    X_poly =  np.concatenate((X_con_poly_scaled,X_cat),axis=1)\n",
    "    \n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRScore:-2.945715159844874e+22\n",
      "RidgeScore:0.9458534218171113\n",
      "LassoScore:0.5561793848668073\n",
      "ElasticNetScore:0.6032948265062534\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "X_train_poly = polyFeatures(X_train_ISO,ConColsNum, CatColsNum)\n",
    "X_test_poly = polyFeatures(X_test_ISO,ConColsNum, CatColsNum)\n",
    "\n",
    "    \n",
    "LR=LinearRegression().fit(X_train_poly,y_train)\n",
    "RG=Ridge().fit(X_train_poly,y_train)\n",
    "LA=Lasso().fit(X_train_poly,y_train)\n",
    "EN=ElasticNet().fit(X_train_poly,y_train)\n",
    "\n",
    "print('LRScore:{}\\nRidgeScore:{}\\nLassoScore:{}\\nElasticNetScore:{}'.format(LR.score(X_test_poly,y_test),RG.score(X_test_poly,y_test),LA.score(X_test_poly,y_test),EN.score(X_test_poly,y_test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, the polynomial features do help to improve the perfermance of Ridge, Lasso and ElasticNetScore. But it does extremely bad on Linear Regression. The reason might be, that the Linear Regression doesn't have regularization term to help get rid of some useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "For this task, we still use the features we use in task 1. And model we used here are GradientBoosting and SVM. Finally, the GradientBoosting has a better perfermance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 0.9381920976713319\n"
     ]
    }
   ],
   "source": [
    "GB = GradientBoostingRegressor().fit(X_train_ISO, y_train)\n",
    "print(\"Gradient Boosting score: {}\".format(GB.score(X_test_ISO, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM grid search score: 0.5993519608175232\n"
     ]
    }
   ],
   "source": [
    "SVR = SVR().fit(X_train_ISO, y_train)\n",
    "print(\"SVM grid search score: {}\".format(SVR.score(X_test_ISO, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "In this task, we use best model, Linear Regression to show how to select the parameters. There are different ways to do that. \n",
    "\n",
    "1. use SelectKBest to select best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.690049698820539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:298: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:303: RuntimeWarning: invalid value encountered in true_divide\n",
      "  F = corr ** 2 / (1 - corr ** 2) * degrees_of_freedom\n",
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/albertzhang/anaconda/envs/python3-5/lib/python3.5/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "select = SelectKBest(k=20, score_func=f_regression)\n",
    "select.fit(X_train_ISO, y_train)\n",
    "X_train_sub = select.transform(X_train_ISO)\n",
    "X_test_sub = select.transform(X_test_ISO)\n",
    "LR_selected = LinearRegression().fit(X_train_sub, y_train)\n",
    "print(\"Test score: {}\".format(LR_selected.score(X_test_sub,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Since this is a linear model, we can just choose the features whose coefficent has a high absolute value than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedColumns = (abs(LR.coef_)>2)[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = X_train_ISO[:,selectedColumns]\n",
    "X_test_selected = X_test_ISO[:,selectedColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716107394381741\n"
     ]
    }
   ],
   "source": [
    "LR_selected = LinearRegression().fit(X_train_selected,y_train)\n",
    "print(LR_selected.score(X_test_selected, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for this model, remove some irrelevant features won't help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
